{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "メタボの分類\n",
    "データ変換\n",
    "回帰\n",
    "スコアの評価\n",
    "データ化\n",
    "\n",
    "関数化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install patsy\n",
    "#! pip install statsmodels\n",
    "#! pip install --upgrade pip\n",
    "#import, def, path\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_pair(target_list, span):#リストから指定の間隔をあけて二つを取り出す\n",
    "    pairs = [[target_list[i], target_list[i + span]] for i in range(len(target_list) - span)]\n",
    "    return pairs\n",
    "def marge_columns(df, columns):#カラムを追加する\n",
    "    mid = pd.DataFrame(index=df.index, columns=columns)\n",
    "    return pd.concat([df, mid], axis=1)\n",
    "def check_metS(df, metS_facter):#メタボ因子を数値化\n",
    "    metS_keys = list(metS_facter.keys())\n",
    "    metS_values = list(metS_facter.values())\n",
    "    df['check_腹囲'] = list(eval(f\"df['{metS_keys[0]}'] {metS_values[0]}\"))\n",
    "    df['check_血圧'] = list(eval(f\"df['{metS_keys[1]}'] {metS_values[1]}\"))\n",
    "    df['check_血糖'] = list(eval(f\"df['{metS_keys[2]}'] {metS_values[2]}\"))\n",
    "    df['check_脂肪'] =list(eval(f\"df['{metS_keys[3]}'] {metS_values[3]}\"))\n",
    "    return df\n",
    "def categorize_facters(df, categorize_facter):# カテゴリー変数を２値化、trueが１、true_valueはリスト\n",
    "    facters = list(categorize_facter.keys())\n",
    "    true_values = list(categorize_facter.values())\n",
    "    for i, true_value in enumerate(true_values):\n",
    "        k += f\"df['{facters[i]}'] == {true_value} |\"\n",
    "    df[f'categorize_{facters[i]}'] = list(eval(k[: -1]))\n",
    "    return df\n",
    "\n",
    "df_name = pd.read_csv('/Volumes/USBDISK/sort_pg/練習用保健データ.csv',  encoding = 'cp932')\n",
    "main_df_path = os.path.join(os.getcwd(), f'{df_name}.csv')\n",
    "target_list = list(range(2015, 2021))\n",
    "span = 4\n",
    "pairs_column = '検診実施年度'\n",
    "metS_facter = {'腹囲':'>= 85', '血圧':'>= 130', '血糖':'>= 110', '脂肪':'>= 150'}\n",
    "metS_keys = list(metS_facter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "df = pd.read_csv(main_df_path)\n",
    "id_column = 'Unnamed: 0'\n",
    "categorize_facter = {'namae':[1,2]}\n",
    "\n",
    "pairs = make_pair(target_list, span, id_column, categorize_facter)\n",
    "for pair in pairs:\n",
    "    df1 = df.query(f'{pairs_column} == {pair[0]}')#train\n",
    "    df1 = check_metS(df1, metS_facter)\n",
    "    df1 = categorize_facters(df1, categorize_facter)\n",
    "    df2 = df.query(f'{pairs_column} == {pair[1]}')#test\n",
    "    df2 = check_metS(df2, metS_facter)\n",
    "    df2 = df2.filter(regex = f'^({id_column}| check_)', axis = 1)#testは必要のないもの以外drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_name\n",
    "#df = marge_columns(df, ['check_腹囲', 'check_血圧', 'check_血糖', 'check_脂肪'])\n",
    "metS_values = list(metS_facter.values())\n",
    "a = '検診実施年度'\n",
    "true_value = [2018, 2019]\n",
    "k = ''\n",
    "for i in true_value:\n",
    "    k += f\"(df['{a}'] == {i}) |\"\n",
    "\n",
    "df[f'check_{a}'] = list(eval(k[:-1]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "各群にrisk_facter_name\n",
    "\n",
    "比べてchange\n",
    "変化した：１\n",
    "変化なし：０\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_name = '睡眠時間'\n",
    "\n",
    "\n",
    "X = df[[column_name]] # 説明変数\n",
    "Y = df['species'].map({'versicolor': 0, 'virginica': 1}) # versicolorをクラス0, virginicaをクラス1とする\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n",
    "\n",
    "lr = LogisticRegression() # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#main\n",
    "\n",
    "df = pd.read_csv(main_df_path)\n",
    "id_column = 'Unnamed: 0'\n",
    "categorize_facter = {'namae':[1,2]}\n",
    "facter_bp = '血圧'\n",
    "bp_threshold = '>= 135'\n",
    "columns_list = list(df.columns)\n",
    "pairs = make_pair(target_list, span, id_column, categorize_facter)\n",
    "for pair in pairs:\n",
    "    df1 = df.query(f'{pairs_column} == {pair[0]}')#train\n",
    "    df1['check_高血圧'] = list(eval(f\"df1['{facter_bp}'] {bp_threshold}\"))\n",
    "    df1 = df1[df1['check_高血圧'] == False]     #元々正常のみ\n",
    "\n",
    "    df2 = df.query(f'{pairs_column} == {pair[1]}')#test\n",
    "    df1['check_高血圧_after'] = list(eval(f\"df2['{facter_bp}'] {bp_threshold}\"))\n",
    "    df2 = df2.filter(regex = f'^({id_column}| check_)', axis = 1)#testは必要のないもの以外drop\n",
    "\n",
    "    df1.join(df2, on = id_column)\n",
    "    df1.dropna(subset=['check_高血圧_after'], inplace = True)\n",
    "    df1['bp_delta'] = df1['check_高血圧_after'] - df1['check_高血圧']\n",
    "\n",
    "    x = df1[['name']] # 説明変数\n",
    "    y = df1['bp_delta'].map({0: 0, 1: 1}) # falseをクラス0, Trueをクラス1とする\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n",
    "\n",
    "\n",
    "    lr = LogisticRegression() # ロジスティック回帰モデルのインスタンスを作成\n",
    "    lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習\n",
    "\n",
    "\n",
    "    df_model[\"偏回帰係数\"] = model.coef_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# データの読み込み\n",
    "exam_data = pd.read_csv(\"exam_data.csv\")\n",
    "# データのグラフ化\n",
    "\n",
    "\n",
    "# モデル化\n",
    "logistic_model = smf.glm(formula = \"result ~ hours\", # 目的変数~説明変数\n",
    "                       data = exam_data, \n",
    "                       family=sm.families.Binomial(link=sm.genmod.families.links.logit())\n",
    "                        )\n",
    "\n",
    "logistic_result = logistic_model.fit()\n",
    "\n",
    "logistic_result.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2乗検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "chi2, p, dof, exp = stats.chi2_contingency(df_name['検診実施年度','市町村名'], correction=False) #correction:イェーツの補正\n",
    "print(\"期待度数\", \"\\n\", exp)\n",
    "print(\"自由度\", \"\\n\", dof)\n",
    "print(\"カイ二乗値\", \"\\n\", chi2)\n",
    "print(\"p値\", \"\\n\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def chi_squeretest(df, facters_list = []):\n",
    "    facters_pairs = [[facters_pair] for facters_pair in combinations(facters_list, 2)]\n",
    "    separate_facter = []\n",
    "    for facters_pair in facters_pairs:\n",
    "        chi2, p, dof, exp = stats.chi2_contingency(df[facters_pair], correction=False) #correction:イェーツの補正\n",
    "        if p < 0.05 :\n",
    "            separate_facter.append(facters_pair)\n",
    "    return separate_facter#[[a,a],[b,b]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "facters_list = ['検診実施年度', '市町村名']\n",
    "categorize_threshhold = 3\n",
    "df = df_name\n",
    "#X2検定\n",
    "from itertools import combinations\n",
    "def chi_squeretest(df, categorize_threshhold = 3, facters_list = []):\n",
    "    categorize_facters_list = [facter for facter in facters_list if len(df[facter].unique()) < categorize_threshhold]\n",
    "    facters_pairs = [facters_pair for facters_pair in combinations(categorize_facters_list, 2)]\n",
    "    separate_facter = []\n",
    "    print(facters_pairs)\n",
    "    for facters_pair in facters_pairs:\n",
    "        cross_tab = pd.crosstab(df[facters_pair[0]], df[facters_pair[1]])\n",
    "        print(cross_tab)\n",
    "        p = stats.chi2_contingency(cross_tab, correction=False)[1] #correction:イェーツの補正\n",
    "        print(p)\n",
    "        if p < 0.05 :\n",
    "            separate_facter.append(facters_pair)\n",
    "    return separate_facter#[[a,a],[b,b]]\n",
    "\n",
    "\n",
    "def fisher_exact_test(df, categorize_threshhold = 3, facters_list = []):\n",
    "    categorize_facters_list = [facter for facter in facters_list if len(df[facter].unique()) < categorize_threshhold]\n",
    "    facters_pairs = [facters_pair for facters_pair in combinations(categorize_facters_list, 2)]\n",
    "    separate_facter = []\n",
    "    for facters_pair in facters_pairs:\n",
    "        cross_tab = pd.crosstab(df[facters_pair[0]], df[facters_pair[1]])\n",
    "        p = stats.fisher_exact(cross_tab)[1] #correction:イェーツの補正\n",
    "        if p < 0.05 :\n",
    "            separate_facter.append(facters_pair)\n",
    "    return separate_facter#[[a,a],[b,b]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フィッシャーの正確確率検定\n",
    "    期待度数が１未満のセルがある\n",
    "    期待度数が５未満のセルが、全体のセルの20%以上ある\n",
    "        時に用いる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数字が大きすぎるため３群以上は不可？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab = pd.crosstab(df[facters_list[0]], df[facters_list[1]])\n",
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = cross_tab.sum().to_list()        #c   \n",
    "row_sums = cross_tab.sum(axis = 1).to_list()   #r\n",
    "full_sum = sum(column_sums)                    #n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "def list_factorial_sum(taraget_list):\n",
    "    sum = 0\n",
    "    for target in taraget_list:\n",
    "        m = math.factorial(target)\n",
    "        sum += m\n",
    "    return sum\n",
    "\n",
    "\n",
    "column_factorial = 0\n",
    "for column_sum in column_sums:\n",
    "    m = math.factorial(column_sum)\n",
    "    column_factorial += m\n",
    "\n",
    "row_factorial = 0\n",
    "for row_sum in row_sums:\n",
    "    m = math.factorial(row_sum)\n",
    "    row_factorial += m\n",
    "\n",
    "n_factorial = math.factorial(full_sum)\n",
    "\n",
    "values_factorial = 0\n",
    "cross_tab_values = list(itertools.chain.from_iterable(cross_tab.values.tolist()))\n",
    "for cross_tab_value in cross_tab_values:\n",
    "    m = math.factorial(cross_tab_value)\n",
    "    values_factorial += m\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_values = cross_tab.values.tolist()\n",
    "for cross_tab_value in cross_tab_values:    #行ごと\n",
    "    rows_value_factorial_sum = 0\n",
    "    for i, cross_tab_value in enumerate(cross_tab_values):\n",
    "        row_value_factorial_sum = list_factorial_sum(cross_tab_value)\n",
    "        row_factorial = math.factorial(column_sums[i])\n",
    "        k = row_value_factorial_sum/row_factorial\n",
    "        rows_value_factorial_sum += k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_value_factorial_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_p = n_factorial/column_factorial\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_p = n_factorial/column_factorial\n",
    "p\n",
    "rows_value_factorial_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_values = cross_tab.values.tolist()\n",
    "for cross_tab_value in cross_tab_values:    #行ごと\n",
    "    rows_value_factorial_sum = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "シャピロウィルク検定\n",
    "　F検定\n",
    "　　Student's t検定\n",
    "　　Welchのt検定\n",
    "　ManホイットニーU検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facter_name = '検診実施年度'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#シャピロウィルク検定\n",
    "from scipy import stats\n",
    "w, p = stats.shapiro(df[facter_name])\n",
    "if p < 0.05:\n",
    "    print('正規分布ではない')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip check\n",
    "\n",
    "def pip_check(pip_name):\n",
    "    import pkg_resources\n",
    "    list_ = [_lib.project_name for _lib in pkg_resources.working_set]\n",
    "    if pip_name not in list_:\n",
    "        ! pip3 install --upgrade pip\n",
    "        ! pip3 install {pip_name}\n",
    "\n",
    "\n",
    "for pip_name in ['pandas', 'statsmodels', 'patsy']:\n",
    "    pip_check('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import master_code\n",
    "for pip_name in ['pip3', 'statsmodels', 'patsy']:\n",
    "    print(pip_name)\n",
    "    master_code.pip_check(pip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "import openpyxl\n",
    "df = pd.read_csv(\"/Volumes/USBDISK/AHHDC/sort_pg/pmc.csv\")\n",
    "#df = pd.read_csv(\"D:\\AHHDC\\sort_pg\\pmc.csv\")\n",
    "aim_facter = '喫煙'\n",
    "\n",
    "\n",
    "def average_delta_check(aim_facter, df, decimal_point = 3, save_xlsx_name = '検定結果'):  #平均値の差があるかどうか\n",
    "    column_names = df.columns.to_list()\n",
    "    data_size = len(df)     ####   \n",
    "    df_hit = df[df[aim_facter] == 1]    #異常あり\n",
    "    df_miss = df[df[aim_facter] == 0]   #異常なし\n",
    "    df_lists = [['column_name', 'method', 'hit_p', 'miss_p', 'method_t', 'ttest_p']]\n",
    "    for column_name in column_names:\n",
    "        if df[column_name].dtypes == 'float':\n",
    "            if len(df[column_name].unique()) < 5:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if data_size > 1000:\n",
    "            method = 'ks'\n",
    "            hit_ks_p = round(stats.kstest(df_hit[column_name], 'norm')[1], decimal_point)\n",
    "            miss_ks_p = round(stats.kstest(df_miss[column_name], 'norm')[1], decimal_point)\n",
    "            if hit_ks_p > 0.05 or miss_ks_p > 0.05:\n",
    "                result = 'not_normal'\n",
    "            else:\n",
    "                result = 'normal'\n",
    "        else:\n",
    "            method = 'shapiro'\n",
    "            hit_shapiro_p = round(stats.shapiro(df_hit[column_name])[1], decimal_point)\n",
    "            miss_shapiro_p = round(stats.shapiro(df_miss[column_name])[1], decimal_point)\n",
    "            if hit_shapiro_p > 0.05 or miss_shapiro_p > 0.05:\n",
    "                result = 'not_normal'\n",
    "            else:\n",
    "                result = 'normal'\n",
    "        if result == 'not_normal':      #正規分布でない場合\n",
    "            mannwhitneyu_p = round(stats.mannwhitneyu(df_hit[column_name], df_miss[column_name])[1], decimal_point)\n",
    "            df_lists.append([column_name, method, eval(r'hit_{0}_p'.format(method)), eval(r'miss_{0}_p'.format(method)), 'Man_U', mannwhitneyu_p])\n",
    "        elif result == 'normal':        #正規分布の場合\n",
    "            f_p = stats.f_oneway(df_hit[column_name], df_miss[column_name])[1]      #F検定、0.05以上ならば等分散\n",
    "            if f_p < 0.05:\n",
    "                method_t = 'welch'\n",
    "                ttest_p = round(stats.ttest_ind(df_hit[column_name], df_miss[column_name], equal_var = False)[1], decimal_point)      #等分散でない場合、ウェルチのt検定\n",
    "            else :\n",
    "                method_t = 'student'\n",
    "                ttest_p = round(stats.ttest_ind(df_hit[column_name], df_miss[column_name], equal_var = True)[1], decimal_point)       #等分散の場合、スチューデントのt検定 \n",
    "            df_lists.append([column_name, method,  eval(r'hit_{0}_p'.format(method)), eval(r'miss_{0}_p'.format(method)), method_t, ttest_p])\n",
    "    #一枚エクセルに書き込み\n",
    "    # save_df = pd.DataFrame(df_lists, columns = ['column_name', 'method', 'hit_p', 'miss_p', 'method_t', 'ttest_p'])\n",
    "    # with pd.ExcelWriter('検定結果.xlsx') as writer:\n",
    "    #     save_df.to_excel(writer, sheet_name = '検定結果')\n",
    "\n",
    "    #excelに書き込み\n",
    "    book = openpyxl.Workbook()\n",
    "    sheet = book['Sheet']\n",
    "    sheet.title = '検定結果'\n",
    "    for y, row_data in enumerate(df_lists):\n",
    "        for x, cell_data in enumerate(row_data):\n",
    "            sheet.cell(row = 1 + y, column = 1 + x, value = df_lists[y][x])\n",
    "    book.save(f'{save_xlsx_name}.xlsx')    \n",
    "\n",
    "def average_delta_check_for_binary_data(df, decimal_point = 3): #０、１カラムを検定\n",
    "    column_names = df.columns.to_list()\n",
    "    for column_name in column_names:\n",
    "        if df[column_name].unique() == [0, 1]:\n",
    "            average_delta_check(aim_facter, df, decimal_point, save_xlsx_name = f'{column_name}検定結果')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_delta_check(aim_facter, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import random\n",
    "random_num1 = norm.rvs(loc = 100, scale = 10, size = 1000)   #平均100、標準偏差10、サイズ1000の正規分布\n",
    "random_num2 = norm.rvs(loc = 110, scale = 10, size = 1000)   #平均110、標準偏差10、サイズ1000の正規分布\n",
    "random_num3 = norm.rvs(loc = 120, scale = 10, size = 1000)   #平均120、標準偏差10、サイズ1000の正規分布\n",
    "random_num4 = norm.rvs(loc = 130, scale = 10, size = 1000)   #平均130、標準偏差10、サイズ1000の正規分布\n",
    "random_num5 = norm.rvs(loc = 100, scale = 15, size = 1000)   #平均140、標準偏差10、サイズ1000の正規分布\n",
    "random_num6 = norm.rvs(loc = 100, scale = 20, size = 1000)   #平均150、標準偏差10、サイズ1000の正規分布\n",
    "random_num7 = norm.rvs(loc = 100, scale = 25, size = 1000)   #平均160、標準偏差10、サイズ1000の正規分布\n",
    "kituenn = [random.randint(0, 1) for i in range(1000)]        #0か1の乱数を1000個生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'random_num1':random_num1, 'random_num2':random_num2, 'random_num3':random_num3, 'random_num4':random_num4, 'random_num5':random_num5, 'random_num6':random_num6, 'random_num7':random_num7, '喫煙':kituenn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_num1</th>\n",
       "      <th>random_num2</th>\n",
       "      <th>random_num3</th>\n",
       "      <th>random_num4</th>\n",
       "      <th>random_num5</th>\n",
       "      <th>random_num6</th>\n",
       "      <th>random_num7</th>\n",
       "      <th>喫煙</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107.270229</td>\n",
       "      <td>117.937989</td>\n",
       "      <td>126.625448</td>\n",
       "      <td>133.362439</td>\n",
       "      <td>104.200852</td>\n",
       "      <td>68.710784</td>\n",
       "      <td>110.698833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94.607739</td>\n",
       "      <td>115.694640</td>\n",
       "      <td>132.831277</td>\n",
       "      <td>135.325708</td>\n",
       "      <td>75.891182</td>\n",
       "      <td>105.014735</td>\n",
       "      <td>116.896481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.748694</td>\n",
       "      <td>125.396945</td>\n",
       "      <td>121.114908</td>\n",
       "      <td>115.623789</td>\n",
       "      <td>115.219077</td>\n",
       "      <td>84.594594</td>\n",
       "      <td>97.893818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.340479</td>\n",
       "      <td>103.179539</td>\n",
       "      <td>131.290530</td>\n",
       "      <td>112.663798</td>\n",
       "      <td>100.985951</td>\n",
       "      <td>79.088857</td>\n",
       "      <td>75.063494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.546142</td>\n",
       "      <td>110.416995</td>\n",
       "      <td>116.544764</td>\n",
       "      <td>134.222420</td>\n",
       "      <td>74.335862</td>\n",
       "      <td>64.877262</td>\n",
       "      <td>88.717049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>107.153321</td>\n",
       "      <td>117.179872</td>\n",
       "      <td>111.240011</td>\n",
       "      <td>132.605904</td>\n",
       "      <td>89.820841</td>\n",
       "      <td>97.259075</td>\n",
       "      <td>125.489396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>101.824697</td>\n",
       "      <td>108.350795</td>\n",
       "      <td>122.301178</td>\n",
       "      <td>117.416930</td>\n",
       "      <td>102.743515</td>\n",
       "      <td>101.422909</td>\n",
       "      <td>77.074711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>100.186142</td>\n",
       "      <td>120.621600</td>\n",
       "      <td>136.161204</td>\n",
       "      <td>130.075395</td>\n",
       "      <td>122.174534</td>\n",
       "      <td>104.405044</td>\n",
       "      <td>120.957229</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>107.730895</td>\n",
       "      <td>119.151205</td>\n",
       "      <td>130.972977</td>\n",
       "      <td>120.239508</td>\n",
       "      <td>98.336834</td>\n",
       "      <td>114.805031</td>\n",
       "      <td>120.984417</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>69.927196</td>\n",
       "      <td>106.681553</td>\n",
       "      <td>114.419261</td>\n",
       "      <td>149.179140</td>\n",
       "      <td>94.503489</td>\n",
       "      <td>127.237592</td>\n",
       "      <td>76.189739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     random_num1  random_num2  random_num3  random_num4  random_num5   \n",
       "0     107.270229   117.937989   126.625448   133.362439   104.200852  \\\n",
       "1      94.607739   115.694640   132.831277   135.325708    75.891182   \n",
       "2     108.748694   125.396945   121.114908   115.623789   115.219077   \n",
       "3     100.340479   103.179539   131.290530   112.663798   100.985951   \n",
       "4      97.546142   110.416995   116.544764   134.222420    74.335862   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "995   107.153321   117.179872   111.240011   132.605904    89.820841   \n",
       "996   101.824697   108.350795   122.301178   117.416930   102.743515   \n",
       "997   100.186142   120.621600   136.161204   130.075395   122.174534   \n",
       "998   107.730895   119.151205   130.972977   120.239508    98.336834   \n",
       "999    69.927196   106.681553   114.419261   149.179140    94.503489   \n",
       "\n",
       "     random_num6  random_num7  喫煙  \n",
       "0      68.710784   110.698833   1  \n",
       "1     105.014735   116.896481   1  \n",
       "2      84.594594    97.893818   1  \n",
       "3      79.088857    75.063494   1  \n",
       "4      64.877262    88.717049   0  \n",
       "..           ...          ...  ..  \n",
       "995    97.259075   125.489396   0  \n",
       "996   101.422909    77.074711   0  \n",
       "997   104.405044   120.957229   1  \n",
       "998   114.805031   120.984417   1  \n",
       "999   127.237592    76.189739   1  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
