{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "メタボの分類\n",
    "データ変換\n",
    "回帰\n",
    "スコアの評価\n",
    "データ化\n",
    "\n",
    "関数化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install patsy\n",
    "#! pip install statsmodels\n",
    "#! pip install --upgrade pip\n",
    "#import, def, path\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_pair(target_list, span):#リストから指定の間隔をあけて二つを取り出す\n",
    "    pairs = [[target_list[i], target_list[i + span]] for i in range(len(target_list) - span)]\n",
    "    return pairs\n",
    "def marge_columns(df, columns):#カラムを追加する\n",
    "    mid = pd.DataFrame(index=df.index, columns=columns)\n",
    "    return pd.concat([df, mid], axis=1)\n",
    "def check_metS(df, metS_facter):#メタボ因子を数値化\n",
    "    metS_keys = list(metS_facter.keys())\n",
    "    metS_values = list(metS_facter.values())\n",
    "    df['check_腹囲'] = list(eval(f\"df['{metS_keys[0]}'] {metS_values[0]}\"))\n",
    "    df['check_血圧'] = list(eval(f\"df['{metS_keys[1]}'] {metS_values[1]}\"))\n",
    "    df['check_血糖'] = list(eval(f\"df['{metS_keys[2]}'] {metS_values[2]}\"))\n",
    "    df['check_脂肪'] =list(eval(f\"df['{metS_keys[3]}'] {metS_values[3]}\"))\n",
    "    return df\n",
    "def categorize_facters(df, categorize_facter):# カテゴリー変数を２値化、trueが１、true_valueはリスト\n",
    "    facters = list(categorize_facter.keys())\n",
    "    true_values = list(categorize_facter.values())\n",
    "    for i, true_value in enumerate(true_values):\n",
    "        k += f\"df['{facters[i]}'] == {true_value} |\"\n",
    "    df[f'categorize_{facters[i]}'] = list(eval(k[: -1]))\n",
    "    return df\n",
    "\n",
    "df_name = pd.read_csv('/Volumes/USBDISK/sort_pg/練習用保健データ.csv',  encoding = 'cp932')\n",
    "main_df_path = os.path.join(os.getcwd(), f'{df_name}.csv')\n",
    "target_list = list(range(2015, 2021))\n",
    "span = 4\n",
    "pairs_column = '検診実施年度'\n",
    "metS_facter = {'腹囲':'>= 85', '血圧':'>= 130', '血糖':'>= 110', '脂肪':'>= 150'}\n",
    "metS_keys = list(metS_facter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "df = pd.read_csv(main_df_path)\n",
    "id_column = 'Unnamed: 0'\n",
    "categorize_facter = {'namae':[1,2]}\n",
    "\n",
    "pairs = make_pair(target_list, span, id_column, categorize_facter)\n",
    "for pair in pairs:\n",
    "    df1 = df.query(f'{pairs_column} == {pair[0]}')#train\n",
    "    df1 = check_metS(df1, metS_facter)\n",
    "    df1 = categorize_facters(df1, categorize_facter)\n",
    "    df2 = df.query(f'{pairs_column} == {pair[1]}')#test\n",
    "    df2 = check_metS(df2, metS_facter)\n",
    "    df2 = df2.filter(regex = f'^({id_column}| check_)', axis = 1)#testは必要のないもの以外drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_name\n",
    "#df = marge_columns(df, ['check_腹囲', 'check_血圧', 'check_血糖', 'check_脂肪'])\n",
    "metS_values = list(metS_facter.values())\n",
    "a = '検診実施年度'\n",
    "true_value = [2018, 2019]\n",
    "k = ''\n",
    "for i in true_value:\n",
    "    k += f\"(df['{a}'] == {i}) |\"\n",
    "\n",
    "df[f'check_{a}'] = list(eval(k[:-1]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "各群にrisk_facter_name\n",
    "\n",
    "比べてchange\n",
    "変化した：１\n",
    "変化なし：０\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_name = '睡眠時間'\n",
    "\n",
    "\n",
    "X = df[[column_name]] # 説明変数\n",
    "Y = df['species'].map({'versicolor': 0, 'virginica': 1}) # versicolorをクラス0, virginicaをクラス1とする\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n",
    "\n",
    "lr = LogisticRegression() # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#main\n",
    "\n",
    "df = pd.read_csv(main_df_path)\n",
    "id_column = 'Unnamed: 0'\n",
    "categorize_facter = {'namae':[1,2]}\n",
    "facter_bp = '血圧'\n",
    "bp_threshold = '>= 135'\n",
    "columns_list = list(df.columns)\n",
    "pairs = make_pair(target_list, span, id_column, categorize_facter)\n",
    "for pair in pairs:\n",
    "    df1 = df.query(f'{pairs_column} == {pair[0]}')#train\n",
    "    df1['check_高血圧'] = list(eval(f\"df1['{facter_bp}'] {bp_threshold}\"))\n",
    "    df1 = df1[df1['check_高血圧'] == False]     #元々正常のみ\n",
    "\n",
    "    df2 = df.query(f'{pairs_column} == {pair[1]}')#test\n",
    "    df1['check_高血圧_after'] = list(eval(f\"df2['{facter_bp}'] {bp_threshold}\"))\n",
    "    df2 = df2.filter(regex = f'^({id_column}| check_)', axis = 1)#testは必要のないもの以外drop\n",
    "\n",
    "    df1.join(df2, on = id_column)\n",
    "    df1.dropna(subset=['check_高血圧_after'], inplace = True)\n",
    "    df1['bp_delta'] = df1['check_高血圧_after'] - df1['check_高血圧']\n",
    "\n",
    "    x = df1[['name']] # 説明変数\n",
    "    y = df1['bp_delta'].map({0: 0, 1: 1}) # falseをクラス0, Trueをクラス1とする\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n",
    "\n",
    "\n",
    "    lr = LogisticRegression() # ロジスティック回帰モデルのインスタンスを作成\n",
    "    lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習\n",
    "\n",
    "\n",
    "    df_model[\"偏回帰係数\"] = model.coef_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# データの読み込み\n",
    "exam_data = pd.read_csv(\"exam_data.csv\")\n",
    "# データのグラフ化\n",
    "\n",
    "\n",
    "# モデル化\n",
    "logistic_model = smf.glm(formula = \"result ~ hours\", # 目的変数~説明変数\n",
    "                       data = exam_data, \n",
    "                       family=sm.families.Binomial(link=sm.genmod.families.links.logit())\n",
    "                        )\n",
    "\n",
    "logistic_result = logistic_model.fit()\n",
    "\n",
    "logistic_result.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2乗検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "chi2, p, dof, exp = stats.chi2_contingency(df_name['検診実施年度','市町村名'], correction=False) #correction:イェーツの補正\n",
    "print(\"期待度数\", \"\\n\", exp)\n",
    "print(\"自由度\", \"\\n\", dof)\n",
    "print(\"カイ二乗値\", \"\\n\", chi2)\n",
    "print(\"p値\", \"\\n\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def chi_squeretest(df, facters_list = []):\n",
    "    facters_pairs = [[facters_pair] for facters_pair in combinations(facters_list, 2)]\n",
    "    separate_facter = []\n",
    "    for facters_pair in facters_pairs:\n",
    "        chi2, p, dof, exp = stats.chi2_contingency(df[facters_pair], correction=False) #correction:イェーツの補正\n",
    "        if p < 0.05 :\n",
    "            separate_facter.append(facters_pair)\n",
    "    return separate_facter#[[a,a],[b,b]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "facters_list = ['検診実施年度', '市町村名']\n",
    "categorize_threshhold = 3\n",
    "df = df_name\n",
    "#X2検定\n",
    "from itertools import combinations\n",
    "def chi_squeretest(df, categorize_threshhold = 3, facters_list = []):\n",
    "    categorize_facters_list = [facter for facter in facters_list if len(df[facter].unique()) < categorize_threshhold]\n",
    "    facters_pairs = [facters_pair for facters_pair in combinations(categorize_facters_list, 2)]\n",
    "    separate_facter = []\n",
    "    print(facters_pairs)\n",
    "    for facters_pair in facters_pairs:\n",
    "        cross_tab = pd.crosstab(df[facters_pair[0]], df[facters_pair[1]])\n",
    "        print(cross_tab)\n",
    "        p = stats.chi2_contingency(cross_tab, correction=False)[1] #correction:イェーツの補正\n",
    "        print(p)\n",
    "        if p < 0.05 :\n",
    "            separate_facter.append(facters_pair)\n",
    "    return separate_facter#[[a,a],[b,b]]\n",
    "\n",
    "\n",
    "def fisher_exact_test(df, categorize_threshhold = 3, facters_list = []):\n",
    "    categorize_facters_list = [facter for facter in facters_list if len(df[facter].unique()) < categorize_threshhold]\n",
    "    facters_pairs = [facters_pair for facters_pair in combinations(categorize_facters_list, 2)]\n",
    "    separate_facter = []\n",
    "    for facters_pair in facters_pairs:\n",
    "        cross_tab = pd.crosstab(df[facters_pair[0]], df[facters_pair[1]])\n",
    "        p = stats.fisher_exact(cross_tab)[1] #correction:イェーツの補正\n",
    "        if p < 0.05 :\n",
    "            separate_facter.append(facters_pair)\n",
    "    return separate_facter#[[a,a],[b,b]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フィッシャーの正確確率検定\n",
    "    期待度数が１未満のセルがある\n",
    "    期待度数が５未満のセルが、全体のセルの20%以上ある\n",
    "        時に用いる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数字が大きすぎるため３群以上は不可？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab = pd.crosstab(df[facters_list[0]], df[facters_list[1]])\n",
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = cross_tab.sum().to_list()        #c   \n",
    "row_sums = cross_tab.sum(axis = 1).to_list()   #r\n",
    "full_sum = sum(column_sums)                    #n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "def list_factorial_sum(taraget_list):\n",
    "    sum = 0\n",
    "    for target in taraget_list:\n",
    "        m = math.factorial(target)\n",
    "        sum += m\n",
    "    return sum\n",
    "\n",
    "\n",
    "column_factorial = 0\n",
    "for column_sum in column_sums:\n",
    "    m = math.factorial(column_sum)\n",
    "    column_factorial += m\n",
    "\n",
    "row_factorial = 0\n",
    "for row_sum in row_sums:\n",
    "    m = math.factorial(row_sum)\n",
    "    row_factorial += m\n",
    "\n",
    "n_factorial = math.factorial(full_sum)\n",
    "\n",
    "values_factorial = 0\n",
    "cross_tab_values = list(itertools.chain.from_iterable(cross_tab.values.tolist()))\n",
    "for cross_tab_value in cross_tab_values:\n",
    "    m = math.factorial(cross_tab_value)\n",
    "    values_factorial += m\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_values = cross_tab.values.tolist()\n",
    "for cross_tab_value in cross_tab_values:    #行ごと\n",
    "    rows_value_factorial_sum = 0\n",
    "    for i, cross_tab_value in enumerate(cross_tab_values):\n",
    "        row_value_factorial_sum = list_factorial_sum(cross_tab_value)\n",
    "        row_factorial = math.factorial(column_sums[i])\n",
    "        k = row_value_factorial_sum/row_factorial\n",
    "        rows_value_factorial_sum += k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_value_factorial_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_p = n_factorial/column_factorial\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_p = n_factorial/column_factorial\n",
    "p\n",
    "rows_value_factorial_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_values = cross_tab.values.tolist()\n",
    "for cross_tab_value in cross_tab_values:    #行ごと\n",
    "    rows_value_factorial_sum = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "シャピロウィルク検定\n",
    "　F検定\n",
    "　　Student's t検定\n",
    "　　Welchのt検定\n",
    "　ManホイットニーU検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facter_name = '検診実施年度'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#シャピロウィルク検定\n",
    "from scipy import stats\n",
    "w, p = stats.shapiro(df[facter_name])\n",
    "if p < 0.05:\n",
    "    print('正規分布ではない')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip check\n",
    "\n",
    "def pip_check(pip_name):\n",
    "    import pkg_resources\n",
    "    list_ = [_lib.project_name for _lib in pkg_resources.working_set]\n",
    "    if pip_name not in list_:\n",
    "        ! pip3 install --upgrade pip\n",
    "        ! pip3 install {pip_name}\n",
    "\n",
    "\n",
    "for pip_name in ['pandas', 'statsmodels', 'patsy']:\n",
    "    pip_check('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import master_code\n",
    "for pip_name in ['pip3', 'statsmodels', 'patsy']:\n",
    "    print(pip_name)\n",
    "    master_code.pip_check(pip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "import openpyxl\n",
    "df = pd.read_csv(\"/Volumes/USBDISK/AHHDC/sort_pg/pmc.csv\")\n",
    "#df = pd.read_csv(\"D:\\AHHDC\\sort_pg\\pmc.csv\")\n",
    "aim_facter = '喫煙'\n",
    "\n",
    "\n",
    "def average_delta_check(aim_facter, df, ):\n",
    "    column_names = df.columns.to_list()\n",
    "    data_size = len(df)     ####   \n",
    "    df_hit = df[df[aim_facter] == 1]    #異常あり\n",
    "    df_miss = df[df[aim_facter] == 0]   #異常なし\n",
    "    df_lists = [['column_name', 'method', 'hit_p', 'miss_p', 'method_t', 'ttest_p']]\n",
    "    for column_name in column_names:\n",
    "        if df[column_name].dtypes == 'float':\n",
    "            if len(df[column_name].unique()) < 5:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if data_size > 1000:\n",
    "            method = 'ks'\n",
    "            hit_ks_p = stats.kstest(df_hit[column_name], 'norm')[1]\n",
    "            miss_ks_p = stats.kstest(df_miss[column_name], 'norm')[1]\n",
    "            if hit_ks_p > 0.05 or miss_ks_p > 0.05:\n",
    "                result = 'not_normal'\n",
    "            else:\n",
    "                result = 'normal'\n",
    "        else:\n",
    "            method = 'shapiro'\n",
    "            hit_shapiro_p = stats.shapiro(df_hit[column_name])[1]\n",
    "            miss_shapiro_p = stats.shapiro(df_miss[column_name])[1]\n",
    "            if hit_shapiro_p > 0.05 or miss_shapiro_p > 0.05:\n",
    "                result = 'not_normal'\n",
    "            else:\n",
    "                result = 'normal'\n",
    "        if result == 'not_normal':      #正規分布でない場合\n",
    "            mannwhitneyu_p = stats.mannwhitneyu(df_hit[column_name], df_miss[column_name])[1]\n",
    "            df_lists.append([column_name, method, eval(r'hit_{0}_p'.format(method)), eval(r'miss_{0}_p'.format(method)), 'Man_U', mannwhitneyu_p])\n",
    "        elif result == 'normal':        #正規分布の場合\n",
    "            f_p = stats.f_oneway(df_hit[column_name], df_miss[column_name])[1]      #F検定、0.05以上ならば等分散\n",
    "            if f_p < 0.05:\n",
    "                method_t = 'welch'\n",
    "                ttest_p = stats.ttest_ind(df_hit[column_name], df_miss[column_name], equal_var = False)[1]      #等分散でない場合、ウェルチのt検定\n",
    "            else :\n",
    "                method_t = 'student'\n",
    "                ttest_p = stats.ttest_ind(df_hit[column_name], df_miss[column_name], equal_var = True)[1]       #等分散の場合、スチューデントのt検定 \n",
    "            df_lists.append([column_name, method,  eval(r'hit_{0}_p'.format(method)), eval(r'miss_{0}_p'.format(method)), method_t, ttest_p])\n",
    "    #save_df = pd.DataFrame(df_lists, columns = ['column_name', 'method', 'hit_p', 'miss_p', 'method_t', 'ttest_p'])\n",
    "\n",
    "    #excelに書き込み\n",
    "    book = openpyxl.Workbook()\n",
    "    sheet = book['Sheet']\n",
    "    sheet.title = '検定結果'\n",
    "    for y, row_data in enumerate(df_lists):\n",
    "        for x, cell_data in enumerate(row_data):\n",
    "            sheet.cell(row = 1 + y, column = 1 + x, value = df_lists[y][x])\n",
    "    book.save('検定結果.xlsx')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_delta_check(aim_facter, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import random\n",
    "random_num1 = norm.rvs(loc = 100, scale = 10, size = 1000)   #平均100、標準偏差10、サイズ1000の正規分布\n",
    "random_num2 = norm.rvs(loc = 110, scale = 10, size = 1000)   #平均110、標準偏差10、サイズ1000の正規分布\n",
    "random_num3 = norm.rvs(loc = 120, scale = 10, size = 1000)   #平均120、標準偏差10、サイズ1000の正規分布\n",
    "random_num4 = norm.rvs(loc = 130, scale = 10, size = 1000)   #平均130、標準偏差10、サイズ1000の正規分布\n",
    "random_num5 = norm.rvs(loc = 100, scale = 15, size = 1000)   #平均140、標準偏差10、サイズ1000の正規分布\n",
    "random_num6 = norm.rvs(loc = 100, scale = 20, size = 1000)   #平均150、標準偏差10、サイズ1000の正規分布\n",
    "random_num7 = norm.rvs(loc = 100, scale = 25, size = 1000)   #平均160、標準偏差10、サイズ1000の正規分布\n",
    "kituenn = [random.randint(0, 1) for i in range(1000)]        #0か1の乱数を1000個生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'random_num1':random_num1, 'random_num2':random_num2, 'random_num3':random_num3, 'random_num4':random_num4, 'random_num5':random_num5, 'random_num6':random_num6, 'random_num7':random_num7, '喫煙':kituenn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
